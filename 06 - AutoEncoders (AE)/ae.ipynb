{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"ml-1m/movies.dat\", sep=\"::\", header=None, engine=\"python\", encoding=\"latin-1\")\n",
    "users = pd.read_csv(\"ml-1m/users.dat\", sep=\"::\", header=None, engine=\"python\", encoding=\"latin-1\")\n",
    "# index::userid::moviesid::rating::timestamp\n",
    "ratings = pd.read_csv(\"ml-1m/ratings.dat\", sep=\"::\", header=None, engine=\"python\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(\"ml-100k/u1.base\", delimiter='\\t')\n",
    "training_set = np.array(training_set, dtype=\"int\")\n",
    "\n",
    "test_set = pd.read_csv(\"ml-100k/u1.test\", delimiter='\\t')\n",
    "test_set = np.array(test_set, dtype=\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the number of users and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users = int(max(max(training_set[:, 0]), max(training_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1]), max(training_set[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the data into an array with users in lines and movies in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_user in range(1, nb_users+1):\n",
    "        id_movies = data[:, 1][data[:, 0] == id_user]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_user]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies-1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the data into Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the architecture of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()        # full connection object\n",
    "        # this second value is number of nodes in first hidden layer ( u need to change it until got a good results :P )\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        # second layer\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        # decode layer 1\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        # last layer\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        \n",
    "        # activation function\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    # x: input vector\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr= 0.01, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 1.7649396793227028\n",
      "epoch 2, loss: 1.0963044769537145\n",
      "epoch 3, loss: 1.0532891870019543\n",
      "epoch 4, loss: 1.038386608166714\n",
      "epoch 5, loss: 1.0309419714735741\n",
      "epoch 6, loss: 1.0266354142213767\n",
      "epoch 7, loss: 1.0239470505780046\n",
      "epoch 8, loss: 1.022075806941828\n",
      "epoch 9, loss: 1.020724849243325\n",
      "epoch 10, loss: 1.0196138553415903\n",
      "epoch 11, loss: 1.018996906047347\n",
      "epoch 12, loss: 1.0183167442264742\n",
      "epoch 13, loss: 1.0179417574507719\n",
      "epoch 14, loss: 1.0173647264412051\n",
      "epoch 15, loss: 1.0172546045104425\n",
      "epoch 16, loss: 1.0169494188770156\n",
      "epoch 17, loss: 1.0167892211837999\n",
      "epoch 18, loss: 1.0163996807381537\n",
      "epoch 19, loss: 1.0163457041278539\n",
      "epoch 20, loss: 1.0161120915962398\n",
      "epoch 21, loss: 1.0160333928084795\n",
      "epoch 22, loss: 1.0160597672517395\n",
      "epoch 23, loss: 1.0159928438744354\n",
      "epoch 24, loss: 1.0159379548190388\n",
      "epoch 25, loss: 1.015713117002033\n",
      "epoch 26, loss: 1.015488205909989\n",
      "epoch 27, loss: 1.0153268591871956\n",
      "epoch 28, loss: 1.014900085776191\n",
      "epoch 29, loss: 1.0136308345372662\n",
      "epoch 30, loss: 1.0112208019040854\n",
      "epoch 31, loss: 1.009188410267297\n",
      "epoch 32, loss: 1.0094169967254552\n",
      "epoch 33, loss: 1.0053415329996787\n",
      "epoch 34, loss: 1.0051588207523996\n",
      "epoch 35, loss: 1.0024371461758423\n",
      "epoch 36, loss: 1.0002270011565315\n",
      "epoch 37, loss: 0.997735353112832\n",
      "epoch 38, loss: 0.9975455443932766\n",
      "epoch 39, loss: 0.993586070874482\n",
      "epoch 40, loss: 0.9950049513607324\n",
      "epoch 41, loss: 0.9900510378221117\n",
      "epoch 42, loss: 0.9903422750273402\n",
      "epoch 43, loss: 0.9864700509426897\n",
      "epoch 44, loss: 0.9857607767114067\n",
      "epoch 45, loss: 0.9833659115424389\n",
      "epoch 46, loss: 0.9835794436103608\n",
      "epoch 47, loss: 0.9788111452002389\n",
      "epoch 48, loss: 0.9792371808314198\n",
      "epoch 49, loss: 0.9757020534238321\n",
      "epoch 50, loss: 0.9798334314819461\n",
      "epoch 51, loss: 0.9764928918657598\n",
      "epoch 52, loss: 0.9723339535570209\n",
      "epoch 53, loss: 0.9725621789493598\n",
      "epoch 54, loss: 0.9814463594769945\n",
      "epoch 55, loss: 0.9739902821315144\n",
      "epoch 56, loss: 0.9809419204708612\n",
      "epoch 57, loss: 0.9789592590204249\n",
      "epoch 58, loss: 0.9768724372835533\n",
      "epoch 59, loss: 0.9747697891561882\n",
      "epoch 60, loss: 0.9714063956518224\n",
      "epoch 61, loss: 0.9668707459883275\n",
      "epoch 62, loss: 0.9671484707890684\n",
      "epoch 63, loss: 0.9621794537971338\n",
      "epoch 64, loss: 0.9650692593534688\n",
      "epoch 65, loss: 0.961134624495249\n",
      "epoch 66, loss: 0.9579543622052288\n",
      "epoch 67, loss: 0.9565312280157977\n",
      "epoch 68, loss: 0.9576268801662812\n",
      "epoch 69, loss: 0.9552350255417383\n",
      "epoch 70, loss: 0.9554123220722938\n",
      "epoch 71, loss: 0.9532365708305659\n",
      "epoch 72, loss: 0.9526067728789563\n",
      "epoch 73, loss: 0.9523213682575555\n",
      "epoch 74, loss: 0.9511879189532166\n",
      "epoch 75, loss: 0.9514533491773335\n",
      "epoch 76, loss: 0.9483147928813017\n",
      "epoch 77, loss: 0.946160855664427\n",
      "epoch 78, loss: 0.9467245664722544\n",
      "epoch 79, loss: 0.9462489240988876\n",
      "epoch 80, loss: 0.9462683129121404\n",
      "epoch 81, loss: 0.9434226693764087\n",
      "epoch 82, loss: 0.9445885040948983\n",
      "epoch 83, loss: 0.943484236767719\n",
      "epoch 84, loss: 0.9427972445354169\n",
      "epoch 85, loss: 0.9409770314220003\n",
      "epoch 86, loss: 0.9422585303919259\n",
      "epoch 87, loss: 0.9403720549028979\n",
      "epoch 88, loss: 0.9407589639555436\n",
      "epoch 89, loss: 0.9389529284947151\n",
      "epoch 90, loss: 0.939357733911323\n",
      "epoch 91, loss: 0.9368305201575676\n",
      "epoch 92, loss: 0.9382377487867148\n",
      "epoch 93, loss: 0.9362357696355615\n",
      "epoch 94, loss: 0.9375537672904186\n",
      "epoch 95, loss: 0.9353594075254035\n",
      "epoch 96, loss: 0.9363796973901056\n",
      "epoch 97, loss: 0.9340541440490415\n",
      "epoch 98, loss: 0.9357453838967218\n",
      "epoch 99, loss: 0.9336079915271389\n",
      "epoch 100, loss: 0.9346743368619052\n",
      "epoch 101, loss: 0.9330238521819608\n",
      "epoch 102, loss: 0.9337388080044183\n",
      "epoch 103, loss: 0.9322990066393487\n",
      "epoch 104, loss: 0.9331782886698821\n",
      "epoch 105, loss: 0.931600426951836\n",
      "epoch 106, loss: 0.9326582444704077\n",
      "epoch 107, loss: 0.9312940058902236\n",
      "epoch 108, loss: 0.9322773041826805\n",
      "epoch 109, loss: 0.9318301565651069\n",
      "epoch 110, loss: 0.9338656870465\n",
      "epoch 111, loss: 0.9299986155183059\n",
      "epoch 112, loss: 0.9313544458692748\n",
      "epoch 113, loss: 0.9299905934050816\n",
      "epoch 114, loss: 0.9302669166906008\n",
      "epoch 115, loss: 0.9290639999372823\n",
      "epoch 116, loss: 0.930318118011601\n",
      "epoch 117, loss: 0.9284754773177141\n",
      "epoch 118, loss: 0.9291451527171618\n",
      "epoch 119, loss: 0.9279643096142736\n",
      "epoch 120, loss: 0.9283550587458179\n",
      "epoch 121, loss: 0.9266010482149136\n",
      "epoch 122, loss: 0.9276444746535675\n",
      "epoch 123, loss: 0.926000772952079\n",
      "epoch 124, loss: 0.9275723839097053\n",
      "epoch 125, loss: 0.925475350041643\n",
      "epoch 126, loss: 0.926382620732055\n",
      "epoch 127, loss: 0.92523797805836\n",
      "epoch 128, loss: 0.9257969961797641\n",
      "epoch 129, loss: 0.924575660020393\n",
      "epoch 130, loss: 0.9252850678633465\n",
      "epoch 131, loss: 0.9238488508163846\n",
      "epoch 132, loss: 0.924396160811945\n",
      "epoch 133, loss: 0.9231000145423521\n",
      "epoch 134, loss: 0.9238237315993264\n",
      "epoch 135, loss: 0.9227245530707845\n",
      "epoch 136, loss: 0.9231211377450417\n",
      "epoch 137, loss: 0.9227843960198185\n",
      "epoch 138, loss: 0.9229087911666148\n",
      "epoch 139, loss: 0.9221446916228402\n",
      "epoch 140, loss: 0.9233035244464395\n",
      "epoch 141, loss: 0.9221464262308279\n",
      "epoch 142, loss: 0.92262448378031\n",
      "epoch 143, loss: 0.9211211262874663\n",
      "epoch 144, loss: 0.9217629317057021\n",
      "epoch 145, loss: 0.9208860784555594\n",
      "epoch 146, loss: 0.9212487468821229\n",
      "epoch 147, loss: 0.9202701039753635\n",
      "epoch 148, loss: 0.9209060146725309\n",
      "epoch 149, loss: 0.9196944711373128\n",
      "epoch 150, loss: 0.9201111151207472\n",
      "epoch 151, loss: 0.9193310079771247\n",
      "epoch 152, loss: 0.9198179956388073\n",
      "epoch 153, loss: 0.9190632790544171\n",
      "epoch 154, loss: 0.9194181127585384\n",
      "epoch 155, loss: 0.9189513486771489\n",
      "epoch 156, loss: 0.9195158841480285\n",
      "epoch 157, loss: 0.9184765269619166\n",
      "epoch 158, loss: 0.9190199376484871\n",
      "epoch 159, loss: 0.9185167876502985\n",
      "epoch 160, loss: 0.9182945314062404\n",
      "epoch 161, loss: 0.9176994781457479\n",
      "epoch 162, loss: 0.9181302518214903\n",
      "epoch 163, loss: 0.9169914150420045\n",
      "epoch 164, loss: 0.9178543511406237\n",
      "epoch 165, loss: 0.9172431699688919\n",
      "epoch 166, loss: 0.9177921500686562\n",
      "epoch 167, loss: 0.9170356866757088\n",
      "epoch 168, loss: 0.9171709571197886\n",
      "epoch 169, loss: 0.9163996838698497\n",
      "epoch 170, loss: 0.917258126861581\n",
      "epoch 171, loss: 0.9178328875247495\n",
      "epoch 172, loss: 0.9164679947402434\n",
      "epoch 173, loss: 0.9156159408322269\n",
      "epoch 174, loss: 0.9157018197189074\n",
      "epoch 175, loss: 0.9156624220671691\n",
      "epoch 176, loss: 0.9161276133506483\n",
      "epoch 177, loss: 0.9156296169581807\n",
      "epoch 178, loss: 0.9159480581956851\n",
      "epoch 179, loss: 0.9151195064433283\n",
      "epoch 180, loss: 0.9151996825647725\n",
      "epoch 181, loss: 0.9150568254669748\n",
      "epoch 182, loss: 0.915572137951786\n",
      "epoch 183, loss: 0.9148426064120275\n",
      "epoch 184, loss: 0.914606340451979\n",
      "epoch 185, loss: 0.9142322283725747\n",
      "epoch 186, loss: 0.9147273000814053\n",
      "epoch 187, loss: 0.9143714038169808\n",
      "epoch 188, loss: 0.9145829531994941\n",
      "epoch 189, loss: 0.9140039933041453\n",
      "epoch 190, loss: 0.9143021003705043\n",
      "epoch 191, loss: 0.9137757572822356\n",
      "epoch 192, loss: 0.9139864834299961\n",
      "epoch 193, loss: 0.9131826319940481\n",
      "epoch 194, loss: 0.914072537893314\n",
      "epoch 195, loss: 0.9131530086869681\n",
      "epoch 196, loss: 0.9132823438689232\n",
      "epoch 197, loss: 0.9122122395792939\n",
      "epoch 198, loss: 0.9130798331207004\n",
      "epoch 199, loss: 0.9125550325102767\n",
      "epoch 200, loss: 0.912910133524243\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.0\n",
    "    for user in range(nb_users):\n",
    "        input = Variable(training_set[user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        # take user that rate at least one movie\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.requires_grad = False\n",
    "            output[ target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 10e-10)\n",
    "            # direction\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.item()*mean_corrector)\n",
    "            s += 1.0\n",
    "            # the amount\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f'epoch {epoch}, loss: {train_loss / s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9490311446100926\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "s = 0.0\n",
    "for user in range(nb_users):\n",
    "    input = Variable(training_set[user]).unsqueeze(0)\n",
    "    target = Variable(test_set[user]).unsqueeze(0)\n",
    "    # take user that rate at least one movie\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.requires_grad = False\n",
    "        output[ target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 10e-10)\n",
    "        test_loss += np.sqrt(loss.item()*mean_corrector)\n",
    "        s += 1.0\n",
    "\n",
    "print(f'test loss: {test_loss / s}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
